{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6af842-484a-4afb-8ffc-70192b9da61f",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/ntl2222/HackathonAI/blob/nikos/topic_extractor.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b9de5-2e93-4913-9e0c-6410a56900d9",
   "metadata": {},
   "source": [
    "# Topic Extraction from an unsupervised dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f32152-9402-4a27-90cb-f5f4b47efd0a",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [gensim.models.LdaMulticore](https://radimrehurek.com/gensim/models/ldamodel.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14888cdf-124d-4680-afd8-fb6483860506",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Data](#Data)\n",
    "- [Text Preprocessing](#Text-Preprocessing)\n",
    "- [Creating Custom Vocabulary](#Creating-Custom-Vocabulary)\n",
    "- [Latent Dirichlet Allocation (LDA) for Topic Extraction](#Latent-Dirichlet-Allocation-(LDA)-for-Topic-Extraction)\n",
    "- [Topic Allocation using FastText](#Topic-Allocation-using-FastText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb26fc-bf46-4a66-ba91-23f3749a0f2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dafd3b74-602d-49b4-8eec-18e24e9decfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b5b2f-1b66-4d7b-8aa4-1cf75d2c6500",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The dataset we used is: [10000 Restaurant Reviews](#https://www.kaggle.com/datasets/joebeachcapital/restaurant-reviews) from www.kaggle.com. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b6ca8942-cad3-4b2b-a2e2-f2f08d20fdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/data.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile scripts/data.py\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def get_data():\n",
    "    dataset_dir = Path('./data/raw')\n",
    "    dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    notEmpty = any(dataset_dir.iterdir())\n",
    "    \n",
    "    if notEmpty:\n",
    "        print('Dataset exists.')\n",
    "        \n",
    "    else:\n",
    "        try:\n",
    "            response = requests.get('https://storage.googleapis.com/kaggle-data-sets/3697155/6410731/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240224%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240224T212811Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=3630dc54d8e2cee4459eceb6d3414ccb669f04b6996660b9d1a5e20d07f242fde686cf4609e222e2e0d4d34746a77c1c0115c550228a80bfb707e252614ae108f6e2b7f6fa206998100df0c3218b91bd5ad6ea64aa2921b4ecb170f123e0e9e36e9e20a0d772e1689d698fa53a1f1f0f673cc4b94b42919f970c6286bd3d2fa7ecf5e72a14a3c4ba8fd32e2074c97e178e922d8a44280914e36b8371ebc172e122d9db33e6bd83735ba3c3f106224e2eb6566d7885fd87dccd26156f7018ec0d1d4138b55b4d27ba205e5fd68e4b923b4ca8b64bced817e37f9164e3284bab015e05ec046bf635f90f18ebf1fcfcc2ab450851c441deea8700d717f33251be3a')\n",
    "        \n",
    "            if response.status_code == 200:\n",
    "                print('Downloading dataset..')\n",
    "                with open('archive.zip', 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "        \n",
    "                print('Unzipping...')\n",
    "                with zipfile.ZipFile('archive.zip', 'r') as zip_ref:\n",
    "                    zip_ref.extractall(dataset_dir)\n",
    "                    print('Done.')\n",
    "        \n",
    "                os.remove('archive.zip')\n",
    "    \n",
    "            else:\n",
    "                raise requests.exceptions.RequestException(f\"Error downloading dataset. status code: {response.status_code}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175dcf27-9c92-498c-a6d7-1ddc0196fc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset exists.\n"
     ]
    }
   ],
   "source": [
    "download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edaaaeeb-5b10-4cfa-ac18-8c615d4b68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path('./data')\n",
    "csv_dir = dataset_dir / 'raw' / 'Restaurant reviews.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eca47744-dd4d-4158-82ff-ada6a8369680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Soumen das and Arun was a great guy. Only beca...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food is good.we ordered Kodi drumsticks and ba...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Madhumathi Mahajan Well to start with nice cou...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>This place has never disappointed us.. The foo...</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Bad rating is mainly because of \"Chicken Bone ...</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>I personally love and prefer Chinese Food. Had...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Checked in here to try some delicious chinese ...</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9955 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review Rating\n",
       "0     The ambience was good, food was quite good . h...      5\n",
       "1     Ambience is too good for a pleasant evening. S...      5\n",
       "2     A must try.. great food great ambience. Thnx f...      5\n",
       "3     Soumen das and Arun was a great guy. Only beca...      5\n",
       "4     Food is good.we ordered Kodi drumsticks and ba...      5\n",
       "...                                                 ...    ...\n",
       "9995  Madhumathi Mahajan Well to start with nice cou...      3\n",
       "9996  This place has never disappointed us.. The foo...    4.5\n",
       "9997  Bad rating is mainly because of \"Chicken Bone ...    1.5\n",
       "9998  I personally love and prefer Chinese Food. Had...      4\n",
       "9999  Checked in here to try some delicious chinese ...    3.5\n",
       "\n",
       "[9955 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(csv_dir, usecols=['Review', 'Rating']).dropna() # make sure we dont have null reviews\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "23082350-eb27-4625-9a65-0d79d64accd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows from column Review if they are not of type string\n",
    "df = df.drop(df[df['Review'].apply(lambda x: not isinstance(x, str))].index)\n",
    "# remove rows that contain only special symbols and not words\n",
    "df = df[~df['Review'].str.contains(r'^[\\W_]+$')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "175c609b-c6f3-45d4-b496-5ff0d26b4059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates:\n",
      "0\n",
      "\n",
      "Missing indexes:\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates or missing values\n",
    "duplicate_index = df.index[df.index.duplicated()]\n",
    "print('Duplicates:')\n",
    "print(len(duplicate_index))\n",
    "\n",
    "print('\\nMissing indexes:')\n",
    "missing_index = set(range(len(df))) - set(df.index)\n",
    "print(len(missing_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "297d55fe-2c46-42ab-a49d-11f1ea461600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing indexes:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df = df.reindex(range(len(df)))\n",
    "missing_index = set(range(len(df))) - set(df.index)\n",
    "df = df.dropna()\n",
    "print('Missing indexes:')\n",
    "print(len(missing_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c635233-8d50-4485-80f4-eacacef52143",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eae1e2a7-9249-4a81-8e77-de97755f3c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re \n",
    "\n",
    "def remove_url(text: str) -> str:\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571edbb4-cc9d-4b6c-ad6f-606668c08f26",
   "metadata": {},
   "source": [
    "* We would also like to handle the emojis that occur in the reviews but without deleting them completely, since they carry a great deal of information in their context. We will instead replace them with the corrensponding text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "842fd700-bba4-40b1-b61c-deba288691b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji\n",
    "\n",
    "def replace_emoji(text: str) -> str:\n",
    "    emojis = demoji.findall(text)\n",
    "\n",
    "    for emoji in emojis:\n",
    "        text = text.replace(emoji, ' ' + emojis[emoji].split(':')[0])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c629dd28-70e3-4327-a998-890d8319d47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "Best place to hangout...ðŸ˜Š\n",
      "Food is really great...\n",
      "Thanks Papiya for the service...ðŸ˜Š\n",
      "Staff was reallly co-operative...\n",
      "Ambience is really great, especially PDR(Private Dining Room) is awesome...ðŸ˜ðŸ‘ŒðŸ»\n",
      "\n",
      "After:\n",
      "Best place to hangout... smiling face with smiling eyes\n",
      "Food is really great...\n",
      "Thanks Papiya for the service... smiling face with smiling eyes\n",
      "Staff was reallly co-operative...\n",
      "Ambience is really great, especially PDR(Private Dining Room) is awesome... smiling face with heart-eyes OK hand\n"
     ]
    }
   ],
   "source": [
    "review = df.values[65][0]\n",
    "\n",
    "print('Before:')\n",
    "print(review)\n",
    "print('\\nAfter:')\n",
    "print(replace_emoji(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d9c492-e189-463c-b0e1-2134c5bd716c",
   "metadata": {},
   "source": [
    "* Next we will perform some standard pre-processing steps (like tokenization, removing stop words, etc.) to prepare our reviews to be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fe31dc4-49cc-4fb7-8728-8357a13a14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    text = text.split(' ')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f422512-c563-42ea-8ed7-9ada400a81e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "The ambience was good, food was quite good . had Saturday lunch , which was cost effective .\n",
      "Good place for a sate brunch. One can also chill with friends and or parents.\n",
      "Waiter Soumen Das was really courteous and helpful.\n",
      "\n",
      "After:\n",
      "['the', 'ambience', 'was', 'good,', 'food', 'was', 'quite', 'good', '.', 'had', 'saturday', 'lunch', ',', 'which', 'was', 'cost', 'effective', '.\\ngood', 'place', 'for', 'a', 'sate', 'brunch.', 'one', 'can', 'also', 'chill', 'with', 'friends', 'and', 'or', 'parents.\\nwaiter', 'soumen', 'das', 'was', 'really', 'courteous', 'and', 'helpful.']\n"
     ]
    }
   ],
   "source": [
    "review, _ = next(iter(df.values))\n",
    "\n",
    "print('Before:')\n",
    "print(review)\n",
    "print('\\nAfter:')\n",
    "print(tokenize(review))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4f23a68-b3d4-4cfc-9c67-2e863e74dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ntlk.download()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text: List[str]) -> List[str]:\n",
    "    text = [words for words in text if words not in stopwords.words('english')]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e385a758-40a3-4460-ac3e-d2553ad8453c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "The ambience was good, food was quite good . had Saturday lunch , which was cost effective .\n",
      "Good place for a sate brunch. One can also chill with friends and or parents.\n",
      "Waiter Soumen Das was really courteous and helpful.\n",
      "\n",
      "After:\n",
      "['ambience', 'good,', 'food', 'quite', 'good', '.', 'saturday', 'lunch', ',', 'cost', 'effective', '.\\ngood', 'place', 'sate', 'brunch.', 'one', 'also', 'chill', 'friends', 'parents.\\nwaiter', 'soumen', 'das', 'really', 'courteous', 'helpful.']\n"
     ]
    }
   ],
   "source": [
    "print('Before:')\n",
    "print(review)\n",
    "print('\\nAfter:')\n",
    "print(remove_stopwords(tokenize(review)))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e301abb1-57f9-4bc5-9ec5-6531882cd786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "483a83f7-4df5-48ef-b5ee-4a7e1ef21879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text: List[str]) -> List[str]:\n",
    "\n",
    "    text = ' '.join(text)\n",
    "    token = sp(text)\n",
    "    text = [word.lemma_ for word in token]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "386b3dfb-9eb7-4a00-973e-ca595219002b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "The ambience was good, food was quite good . had Saturday lunch , which was cost effective .\n",
      "Good place for a sate brunch. One can also chill with friends and or parents.\n",
      "Waiter Soumen Das was really courteous and helpful.\n",
      "\n",
      "After:\n",
      "['the', 'ambience', 'be', 'good', ',', 'food', 'be', 'quite', 'good', '.', 'have', 'saturday', 'lunch', ',', 'which', 'be', 'cost', 'effective', '.', '\\n', 'good', 'place', 'for', 'a', 'sate', 'brunch', '.', 'one', 'can', 'also', 'chill', 'with', 'friend', 'and', 'or', 'parent', '.', '\\n', 'waiter', 'soumen', 'das', 'be', 'really', 'courteous', 'and', 'helpful', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Before:')\n",
    "print(review)\n",
    "print('\\nAfter:')\n",
    "print(lemmatization(tokenize(review)))     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8c12f-ea6c-4ebc-89f3-7a00727452c2",
   "metadata": {},
   "source": [
    "* Now putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f828da0-0409-41f1-902a-a09249bca170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile src/utils/text_clean.py\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "import demoji\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "class TextCleaner():\n",
    "    '''Performs various transformation to a string to prepare it for nlp.\n",
    "        \n",
    "        Example usage:\n",
    "            f = TextCleaner(remove_verbs=False)\n",
    "            clean_text = f.clean('this is an example text')'''\n",
    "    \n",
    "    def __init__(self, remove_stopwords: bool = True, remove_verbs: bool = True, apply_lemma: bool = True):\n",
    "        self.remove_verbs = remove_verbs\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.apply_lemma = apply_lemma\n",
    "        self.tokens = []\n",
    "\n",
    "        self.sp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "    def tokenizer(self, text: str) -> List[str]:\n",
    "        '''Transforms input text to lowercase and splits it to tokens'''\n",
    "        doc = self.sp(text)\n",
    "        tokens = []\n",
    "\n",
    "        for token in doc:\n",
    "            if self.remove_verbs and not token.pos_.startswith('N'):\n",
    "                continue\n",
    "        \n",
    "            if self.remove_stopwords and token.text.lower() in stopwords.words('english'):\n",
    "                continue\n",
    "\n",
    "            # Check if the token is not a punctuation or whitespace and is not empty\n",
    "            if not token.is_punct and not token.is_space and token.text.strip():          \n",
    "                tokens.append(token.text.lower())\n",
    "        return tokens\n",
    "\n",
    "    def lemmatize(self, text: str) -> List[str]:\n",
    "        '''Returns lemmatized tokens if apply_lemma = True'''\n",
    "        doc = self.sp(text)\n",
    "        tokens = []\n",
    "\n",
    "        for token in doc:\n",
    "            if self.remove_verbs and not token.pos_.startswith('N'):\n",
    "                continue\n",
    "        \n",
    "            if self.remove_stopwords and token.text.lower() in stopwords.words('english'):\n",
    "                continue\n",
    "                \n",
    "            # Check if the token is not a punctuation or whitespace and is not empty\n",
    "            if not token.is_punct and not token.is_space and token.text.strip():\n",
    "                lemma_token = token.lemma_.lower()\n",
    "                tokens.append(lemma_token)\n",
    "        return tokens\n",
    "\n",
    "    def _demoji_replace(self, text: str) -> str:\n",
    "        '''Replaces emojis with text'''\n",
    "        emojis = demoji.findall(text)\n",
    "        for emoji in emojis:\n",
    "            text = text.replace(emoji, ' ' + emojis[emoji].split(':')[0])    \n",
    "        return text\n",
    "\n",
    "    def clean(self, text: str) -> str:\n",
    "        '''Performs a full transformation of the input text'''\n",
    "        # Remove urls\n",
    "        clean_text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        # Replace emojis\n",
    "        clean_text = self._demoji_replace(clean_text)\n",
    "        # Tokenize & lemmatization\n",
    "        if self.apply_lemma:\n",
    "            self.tokens = self.lemmatize(clean_text)\n",
    "        else:\n",
    "            self.tokens = self.tokenizer(clean_text)\n",
    "            \n",
    "        # Join tokens back into a single string\n",
    "        cleaned_text = \" \".join(self.tokens)\n",
    "        # self.tokens = tokens\n",
    "        return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "78c7d84c-12cc-4a68-a927-a37c615beeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'] = df['Review'].astype(str)\n",
    "\n",
    "# f = TextCleaner(remove_verbs=True)\n",
    "# df['cleaned-reviews'] = df['Review'].map(lambda review: f.clean(review))\n",
    "# df.to_csv(dataset_dir / 'processed' /'clean_no_verbs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667228fd-6b0a-438c-8b7f-76796433c5ba",
   "metadata": {},
   "source": [
    "# Creating Custom Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "962216c3-cd0f-4ed6-b964-d088a716b2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>cleaned-reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>5</td>\n",
       "      <td>ambience food lunch cost place sate brunch fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>5</td>\n",
       "      <td>ambience evening service food experience kudo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>5</td>\n",
       "      <td>food ambience service recommendation music bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Soumen das and Arun was a great guy. Only beca...</td>\n",
       "      <td>5</td>\n",
       "      <td>guy behavior sincerety food course place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food is good.we ordered Kodi drumsticks and ba...</td>\n",
       "      <td>5</td>\n",
       "      <td>food drumstick basket biryani thank ambience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>I am amazed at the quality of food and service...</td>\n",
       "      <td>4</td>\n",
       "      <td>quality food service place ambience location p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9884</th>\n",
       "      <td>The food was amazing. Do not forget to try 'Mo...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>food sizzler staff chicken town heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9885</th>\n",
       "      <td>We ordered from here via swiggy:\\n\\nWe ordered...</td>\n",
       "      <td>4</td>\n",
       "      <td>swiggy mushroom quantity dish paneer gravy dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9886</th>\n",
       "      <td>I have been to this place on a sunday with my ...</td>\n",
       "      <td>1</td>\n",
       "      <td>place friend meal time friend moment 2:15pm ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9887</th>\n",
       "      <td>Saturday afternoon we decided to try out this ...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>afternoon place ambience place bowl chicken ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9888 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review Rating  \\\n",
       "0     The ambience was good, food was quite good . h...      5   \n",
       "1     Ambience is too good for a pleasant evening. S...      5   \n",
       "2     A must try.. great food great ambience. Thnx f...      5   \n",
       "3     Soumen das and Arun was a great guy. Only beca...      5   \n",
       "4     Food is good.we ordered Kodi drumsticks and ba...      5   \n",
       "...                                                 ...    ...   \n",
       "9883  I am amazed at the quality of food and service...      4   \n",
       "9884  The food was amazing. Do not forget to try 'Mo...    4.5   \n",
       "9885  We ordered from here via swiggy:\\n\\nWe ordered...      4   \n",
       "9886  I have been to this place on a sunday with my ...      1   \n",
       "9887  Saturday afternoon we decided to try out this ...    4.5   \n",
       "\n",
       "                                        cleaned-reviews  \n",
       "0     ambience food lunch cost place sate brunch fri...  \n",
       "1     ambience evening service food experience kudo ...  \n",
       "2     food ambience service recommendation music bac...  \n",
       "3              guy behavior sincerety food course place  \n",
       "4          food drumstick basket biryani thank ambience  \n",
       "...                                                 ...  \n",
       "9883  quality food service place ambience location p...  \n",
       "9884              food sizzler staff chicken town heart  \n",
       "9885  swiggy mushroom quantity dish paneer gravy dis...  \n",
       "9886  place friend meal time friend moment 2:15pm ma...  \n",
       "9887  afternoon place ambience place bowl chicken ch...  \n",
       "\n",
       "[9888 rows x 3 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(dataset_dir / 'processed' /'clean_no_verbs.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dbe7bdaa-d711-4e15-b5c1-75ec28a3db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'cleaned-reviews'] = df['cleaned-reviews'].astype(str)\n",
    "reviews = df['cleaned-reviews'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2843962f-aac5-4c5f-b8aa-dcd1b17245e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile src/utils/vocab.py\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import vocab\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "class CustomVocab(torchtext.vocab.Vocab):\n",
    "    '''Creates a custom vocabulary from a list of strings and provides various information \n",
    "        about it in the form of attributes.'''\n",
    "    \n",
    "    def __init__(self, document: Union[List[str], str]):\n",
    "        super(CustomVocab, self).__init__(None)\n",
    "        \n",
    "        self.rawText = document\n",
    "        self.tokens = []\n",
    "        self.word_freqs = []\n",
    "        self.vocab = self._create_vocab(document)\n",
    "        self.size = len(self.word_freqs)\n",
    "        self.id2word = []\n",
    "        self.bow = self._bag_of_words(document)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_freqs)\n",
    "\n",
    "    def _create_vocab(self, document):\n",
    "        tokens = self._get_tokens(document)\n",
    "        orderedDict = self._get_word_freq(tokens)\n",
    "        \n",
    "        vocab = torchtext.vocab.vocab(ordered_dict=orderedDict, min_freq=1)\n",
    "        vocab.set_default_index(-1)\n",
    "        return vocab\n",
    "\n",
    "    def _get_tokens(self, document):\n",
    "        if isinstance(document, str):\n",
    "            document = [document]\n",
    "\n",
    "        tokens = []\n",
    "        for word in document:\n",
    "            token = word.split(' ')\n",
    "            tokens.extend(token)\n",
    "\n",
    "        self.tokens = tokens\n",
    "        return tokens\n",
    "\n",
    "    def _get_word_freq(self, tokens):       \n",
    "\n",
    "        counter = Counter(tokens)\n",
    "        sort_counter = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.word_freqs = sort_counter\n",
    "        return OrderedDict(counter)\n",
    "\n",
    "    def _bag_of_words(self, document):\n",
    "        words = []\n",
    "        for doc in document:\n",
    "            words.append([token for token in doc.split(' ')])\n",
    "    \n",
    "        self.id2word = corpora.Dictionary(words)\n",
    "        \n",
    "        return [self.id2word.doc2bow(word) for word in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d76e93b-2910-49c7-bc69-cc0fa8c1d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = CustomVocab(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b193b296-f83d-474e-b4a0-b78dec957ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vocab.tokens\n",
    "freq = vocab.word_freqs\n",
    "stoi = vocab.get_stoi()\n",
    "bow = vocab.bow\n",
    "id2word = vocab.id2word\n",
    "vocab_size = vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d093f5cb-8a23-47e3-8e89-8201e7981a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9888, 6944)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow), len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c8ccef05-dca8-4e92-a0fa-92c59e373088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6944"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d98c8-31c2-48ff-8797-0d0208b39281",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA) for Topic Extraction\n",
    "We use an LDA model from gensim library to find relevant topics in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "48d677eb-fe21-4cf6-a8f3-627cc01ffc41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%writefile src/models/lda_model.py\n",
    "\n",
    "from gensim.models import LdaMulticore\n",
    "from src.utils.vocab import CustomVocab\n",
    "\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "def extraxt_topics(document: List[str],\n",
    "                    num_topics: int = 3,\n",
    "                    passes: int = 1,\n",
    "                    iterations: int = 100) -> Dict[int, str]:\n",
    "    '''Initializes and trains the lda model.\n",
    "       Returns the top n topics of the document.'''\n",
    "\n",
    "    vocab = CustomVocab(document)\n",
    "    id2word = vocab.id2word\n",
    "    bow = vocab.bow\n",
    "    \n",
    "\n",
    "    num_cores = os.cpu_count()\n",
    "\n",
    "    # initialize and train lda model\n",
    "    lda_model = LdaMulticore(corpus=bow, id2word=id2word,\n",
    "                             num_topics=1,\n",
    "                             passes=passes,\n",
    "                             iterations=iterations,\n",
    "                             workers=num_cores)\n",
    "                             \n",
    "    lda_model.save('data/saved_models/lda_model')\n",
    "\n",
    "    # get the top n topics\n",
    "    labels = {id: topic[0] for id, topic in enumerate(lda_model.show_topic(0, topn=num_topics))}\n",
    "\n",
    "    return labels\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7cfd3ed3-5768-46d2-8e7d-0756191a3918",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = extraxt_topics(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8a929f29-535b-4403-91cf-52cddbed7aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food\n",
      "place\n",
      "service\n"
     ]
    }
   ],
   "source": [
    "for _, topic in labels.items():\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d9281-bb48-453a-a030-7da8eb55016e",
   "metadata": {},
   "source": [
    "# Topic Allocation using FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "95232fdb-55c6-489b-a127-678895495531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/models/fastText_model.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/models/fastText_model.py\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def train_model():\n",
    "    '''Initialize and train a FastText model.'''\n",
    "    # initialize\n",
    "    fasttext_model = FastText(vector_size=100, window=3, min_count=1, workers=os.cpu_count(), sg=0)\n",
    "    fasttext_model.build_vocab_from_freq(word_freq=dict(freq))\n",
    "    # train & save model\n",
    "    fasttext_model.train(corpus_iterable=reviews, total_examples=len(reviews), epochs=20)\n",
    "    # fasttext_model.save('data/saved_models/fastText_model')\n",
    "\n",
    "    return fasttext_model\n",
    "\n",
    "\n",
    "def match_topics(document: List[str], labels: Dict[int, str]) -> Dict[str, str]:\n",
    "    '''Matches each review to the most relevant topic.'''\n",
    "    topic_dict = {}\n",
    "    for row, review in enumerate(reviews):\n",
    "        prob = []\n",
    "        for _, topic in labels.items():\n",
    "            prob.append(fasttext_model.wv.n_similarity(review, topic))\n",
    "            topic_dict[review] = labels[np.argmax(prob)]\n",
    "            \n",
    "    return topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8d987c10-332a-49c2-a308-2f40cbaad952",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = train_model()\n",
    "topic_dict = match_topics(reviews, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "986d5ead-37d6-41b3-b5dd-b6c005230aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned-reviews</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ambience food lunch cost place sate brunch fri...</td>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ambience evening service food experience kudo ...</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food ambience service recommendation music bac...</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>guy behavior sincerety food course place</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>food drumstick basket biryani thank ambience</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8564</th>\n",
       "      <td>quality food service place ambience location p...</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8565</th>\n",
       "      <td>food sizzler staff chicken town heart</td>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8566</th>\n",
       "      <td>swiggy mushroom quantity dish paneer gravy dis...</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8567</th>\n",
       "      <td>place friend meal time friend moment 2:15pm ma...</td>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8568</th>\n",
       "      <td>afternoon place ambience place bowl chicken ch...</td>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8569 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned-reviews    topic\n",
       "0     ambience food lunch cost place sate brunch fri...    place\n",
       "1     ambience evening service food experience kudo ...  service\n",
       "2     food ambience service recommendation music bac...  service\n",
       "3              guy behavior sincerety food course place  service\n",
       "4          food drumstick basket biryani thank ambience  service\n",
       "...                                                 ...      ...\n",
       "8564  quality food service place ambience location p...  service\n",
       "8565              food sizzler staff chicken town heart    place\n",
       "8566  swiggy mushroom quantity dish paneer gravy dis...  service\n",
       "8567  place friend meal time friend moment 2:15pm ma...    place\n",
       "8568  afternoon place ambience place bowl chicken ch...    place\n",
       "\n",
       "[8569 rows x 2 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(topic_dict.items())\n",
    "topic_df = pd.DataFrame(data=data, columns=['cleaned-reviews', 'topic'], dtype=str)\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8e80b6e8-6b4a-481e-8b73-920426e7f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df.to_csv(dataset_dir / 'processed' / 'topics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon-kernel",
   "language": "python",
   "name": "hackathon-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
