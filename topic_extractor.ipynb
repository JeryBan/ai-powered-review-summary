{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6af842-484a-4afb-8ffc-70192b9da61f",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/ntl2222/HackathonAI/blob/nikos/topic_extractor.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b9de5-2e93-4913-9e0c-6410a56900d9",
   "metadata": {},
   "source": [
    "# Topic Extraction from an unsupervised dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f32152-9402-4a27-90cb-f5f4b47efd0a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e2d5f-d9e1-4d91-b8c5-55e33e7b2af3",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14888cdf-124d-4680-afd8-fb6483860506",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Data](#Data)\n",
    "  - [Preprocessing](#Preprocessing)\n",
    "- [Topic Extraction using FastText](#Topic-Extraction-using-FastText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb26fc-bf46-4a66-ba91-23f3749a0f2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dafd3b74-602d-49b4-8eec-18e24e9decfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b5b2f-1b66-4d7b-8aa4-1cf75d2c6500",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The dataset we used is: [10000 Restaurant Reviews](#https://www.kaggle.com/datasets/joebeachcapital/restaurant-reviews) from www.kaggle.com. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6ca8942-cad3-4b2b-a2e2-f2f08d20fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile scripts/get_data.py\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def download_data():\n",
    "    dataset_dir = Path('./data/raw')\n",
    "    dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    notEmpty = any(dataset_dir.iterdir())\n",
    "    \n",
    "    if notEmpty:\n",
    "        print('Dataset exists.')\n",
    "        \n",
    "    else:\n",
    "        try:\n",
    "            response = requests.get('https://storage.googleapis.com/kaggle-data-sets/3697155/6410731/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240224%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240224T212811Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=3630dc54d8e2cee4459eceb6d3414ccb669f04b6996660b9d1a5e20d07f242fde686cf4609e222e2e0d4d34746a77c1c0115c550228a80bfb707e252614ae108f6e2b7f6fa206998100df0c3218b91bd5ad6ea64aa2921b4ecb170f123e0e9e36e9e20a0d772e1689d698fa53a1f1f0f673cc4b94b42919f970c6286bd3d2fa7ecf5e72a14a3c4ba8fd32e2074c97e178e922d8a44280914e36b8371ebc172e122d9db33e6bd83735ba3c3f106224e2eb6566d7885fd87dccd26156f7018ec0d1d4138b55b4d27ba205e5fd68e4b923b4ca8b64bced817e37f9164e3284bab015e05ec046bf635f90f18ebf1fcfcc2ab450851c441deea8700d717f33251be3a')\n",
    "        \n",
    "            if response.status_code == 200:\n",
    "                print('Downloading dataset..')\n",
    "                with open('archive.zip', 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "        \n",
    "                print('Unzipping...')\n",
    "                with zipfile.ZipFile('archive.zip', 'r') as zip_ref:\n",
    "                    zip_ref.extractall(dataset_dir)\n",
    "                    print('Done.')\n",
    "        \n",
    "                os.remove('archive.zip')\n",
    "    \n",
    "            else:\n",
    "                raise requests.exceptions.RequestException(f\"Error downloading dataset. status code: {response.status_code}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "175dcf27-9c92-498c-a6d7-1ddc0196fc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset exists.\n"
     ]
    }
   ],
   "source": [
    "download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edaaaeeb-5b10-4cfa-ac18-8c615d4b68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = dataset_dir / 'Restaurant-reviews.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca47744-dd4d-4158-82ff-ada6a8369680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique rows: 9954\n",
      "                                                 Review Rating\n",
      "0     The ambience was good, food was quite good . h...      5\n",
      "1     Ambience is too good for a pleasant evening. S...      5\n",
      "2     A must try.. great food great ambience. Thnx f...      5\n",
      "3     Soumen das and Arun was a great guy. Only beca...      5\n",
      "4     Food is good.we ordered Kodi drumsticks and ba...      5\n",
      "...                                                 ...    ...\n",
      "9994  Madhumathi Mahajan Well to start with nice cou...      3\n",
      "9995  This place has never disappointed us.. The foo...    4.5\n",
      "9996  Bad rating is mainly because of \"Chicken Bone ...    1.5\n",
      "9997  I personally love and prefer Chinese Food. Had...      4\n",
      "9998  Checked in here to try some delicious chinese ...    3.5\n",
      "\n",
      "[9954 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(csv_dir, usecols=['Review', 'Rating']).dropna() # make sure we dont have null reviews\n",
    "print(f'Unique rows: {df.index.nunique()}')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23082350-eb27-4625-9a65-0d79d64accd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows from column Review if they are not of type string\n",
    "df = df.drop(df[df['Review'].apply(lambda x: not isinstance(x, str))].index)\n",
    "# remove rows that contain only special symbols and not words\n",
    "df = df[~df['Review'].str.contains(r'^[\\W_]+$')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "175c609b-c6f3-45d4-b496-5ff0d26b4059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates:\n",
      "0\n",
      "\n",
      "Missing indexes:\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates or missing values\n",
    "duplicate_index = df.index[df.index.duplicated()]\n",
    "print('Duplicates:')\n",
    "print(len(duplicate_index))\n",
    "\n",
    "print('\\nMissing indexes:')\n",
    "missing_index = set(range(len(df))) - set(df.index)\n",
    "print(len(missing_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "297d55fe-2c46-42ab-a49d-11f1ea461600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing indexes:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df = df.reindex(range(len(df)))\n",
    "missing_index = set(range(len(df))) - set(df.index)\n",
    "df = df.dropna()\n",
    "print('Missing indexes:')\n",
    "print(len(missing_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c635233-8d50-4485-80f4-eacacef52143",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eae1e2a7-9249-4a81-8e77-de97755f3c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re \n",
    "\n",
    "def remove_url(text: str) -> str:\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571edbb4-cc9d-4b6c-ad6f-606668c08f26",
   "metadata": {},
   "source": [
    "* We would also like to handle the emojis that occur in the reviews but without deleting them completely, since they carry a great deal of information in their context. We will instead replace them with the corrensponding text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "842fd700-bba4-40b1-b61c-deba288691b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji\n",
    "\n",
    "def replace_emoji(text: str) -> str:\n",
    "    emojis = demoji.findall(text)\n",
    "\n",
    "    for emoji in emojis:\n",
    "        text = text.replace(emoji, ' ' + emojis[emoji].split(':')[0])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c629dd28-70e3-4327-a998-890d8319d47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "Best place to hangout...ðŸ˜Š\n",
      "Food is really great...\n",
      "Thanks Papiya for the service...ðŸ˜Š\n",
      "Staff was reallly co-operative...\n",
      "Ambience is really great, especially PDR(Private Dining Room) is awesome...ðŸ˜ðŸ‘ŒðŸ»\n",
      "\n",
      "After:\n",
      "Best place to hangout... smiling face with smiling eyes\n",
      "Food is really great...\n",
      "Thanks Papiya for the service... smiling face with smiling eyes\n",
      "Staff was reallly co-operative...\n",
      "Ambience is really great, especially PDR(Private Dining Room) is awesome... smiling face with heart-eyes OK hand\n"
     ]
    }
   ],
   "source": [
    "review = df.values[65][0]\n",
    "\n",
    "print('Before:')\n",
    "print(review)\n",
    "print('\\nAfter:')\n",
    "print(replace_emoji(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d9c492-e189-463c-b0e1-2134c5bd716c",
   "metadata": {},
   "source": [
    "* Next we will perform some standard pre-processing steps (like tokenization, removing stop words, etc.) to prepare our reviews to be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fe31dc4-49cc-4fb7-8728-8357a13a14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    text = text.split(' ')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f422512-c563-42ea-8ed7-9ada400a81e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "The ambience was good, food was quite good . had Saturday lunch , which was cost effective .\n",
      "Good place for a sate brunch. One can also chill with friends and or parents.\n",
      "Waiter Soumen Das was really courteous and helpful.\n",
      "\n",
      "After:\n",
      "['the', 'ambience', 'was', 'good,', 'food', 'was', 'quite', 'good', '.', 'had', 'saturday', 'lunch', ',', 'which', 'was', 'cost', 'effective', '.\\ngood', 'place', 'for', 'a', 'sate', 'brunch.', 'one', 'can', 'also', 'chill', 'with', 'friends', 'and', 'or', 'parents.\\nwaiter', 'soumen', 'das', 'was', 'really', 'courteous', 'and', 'helpful.']\n"
     ]
    }
   ],
   "source": [
    "review, _ = next(iter(df.values))\n",
    "\n",
    "print('Before:')\n",
    "print(review)\n",
    "print('\\nAfter:')\n",
    "print(tokenize(review))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4f23a68-b3d4-4cfc-9c67-2e863e74dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ntlk.download()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text: List[str]) -> List[str]:\n",
    "    text = [words for words in text if words not in stopwords.words('english')]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e385a758-40a3-4460-ac3e-d2553ad8453c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "The ambience was good, food was quite good . had Saturday lunch , which was cost effective .\n",
      "Good place for a sate brunch. One can also chill with friends and or parents.\n",
      "Waiter Soumen Das was really courteous and helpful.\n",
      "\n",
      "After:\n",
      "['ambience', 'good,', 'food', 'quite', 'good', '.', 'saturday', 'lunch', ',', 'cost', 'effective', '.\\ngood', 'place', 'sate', 'brunch.', 'one', 'also', 'chill', 'friends', 'parents.\\nwaiter', 'soumen', 'das', 'really', 'courteous', 'helpful.']\n"
     ]
    }
   ],
   "source": [
    "print('Before:')\n",
    "print(review)\n",
    "print('\\nAfter:')\n",
    "print(remove_stopwords(tokenize(review)))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e301abb1-57f9-4bc5-9ec5-6531882cd786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "483a83f7-4df5-48ef-b5ee-4a7e1ef21879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text: List[str]) -> List[str]:\n",
    "\n",
    "    text = ' '.join(text)\n",
    "    token = sp(text)\n",
    "    text = [word.lemma_ for word in token]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "386b3dfb-9eb7-4a00-973e-ca595219002b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "The ambience was good, food was quite good . had Saturday lunch , which was cost effective .\n",
      "Good place for a sate brunch. One can also chill with friends and or parents.\n",
      "Waiter Soumen Das was really courteous and helpful.\n",
      "\n",
      "After:\n",
      "['the', 'ambience', 'be', 'good', ',', 'food', 'be', 'quite', 'good', '.', 'have', 'saturday', 'lunch', ',', 'which', 'be', 'cost', 'effective', '.', '\\n', 'good', 'place', 'for', 'a', 'sate', 'brunch', '.', 'one', 'can', 'also', 'chill', 'with', 'friend', 'and', 'or', 'parent', '.', '\\n', 'waiter', 'soumen', 'das', 'be', 'really', 'courteous', 'and', 'helpful', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Before:')\n",
    "print(review)\n",
    "print('\\nAfter:')\n",
    "print(lemmatization(tokenize(review)))     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8c12f-ea6c-4ebc-89f3-7a00727452c2",
   "metadata": {},
   "source": [
    "* Now putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f828da0-0409-41f1-902a-a09249bca170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import demoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "class TextCleaner():\n",
    "    def __init__(self, text: str = None, remove_stops: bool = True, remove_verbs: bool = False):\n",
    "        self.remove_verbs = remove_verbs\n",
    "        self.remove_stops = remove_stops\n",
    "        self.tokens = []\n",
    "\n",
    "        sp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "    def tokenizer(self, text: str) -> List[str]:\n",
    "        '''Transforms input text to lowercase and splits it to tokens.\n",
    "           Applies lemmatization if lemma=True'''\n",
    "        doc = sp(text)\n",
    "        tokens = []\n",
    "\n",
    "        for token in doc:\n",
    "            # Check if the token is not a punctuation or whitespace and is not empty\n",
    "            if not token.is_punct and not token.is_space and token.text.strip():\n",
    "                # Check if the token is a noun\n",
    "                if self.remove_verbs:\n",
    "                    if token.pos_.startswith('N'):                        \n",
    "                        lemma_token = token.lemma_.lower()\n",
    "                        tokens.append(lemma_token)\n",
    "                    elif not token.pos_.startswith('N'):\n",
    "                        continue   \n",
    "                else:\n",
    "                    lemma_token = token.lemma_.lower()\n",
    "                    tokens.append(lemma_token)                \n",
    "    \n",
    "        return tokens\n",
    "\n",
    "    def _remove_stopwords(self, tokens: List[str]) -> List[str]:\n",
    "        '''Removes stop words'''\n",
    "        return [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "    def _demoji_replace(self, text: str) -> str:\n",
    "        '''Replaces emojis with text'''\n",
    "        emojis = demoji.findall(text)\n",
    "        for emoji in emojis:\n",
    "            text = text.replace(emoji, ' ' + emojis[emoji].split(':')[0])\n",
    "    \n",
    "        return text\n",
    "\n",
    "    def clean(self, text: str) -> str:\n",
    "        '''Performs a full transformation of the input text'''\n",
    "        # Remove urls\n",
    "        clean_text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        # Replace emojis\n",
    "        clean_text = self._demoji_replace(clean_text)\n",
    "        # Remove punctuation\n",
    "        clean_text = re.sub(r'[^\\w\\s]', '', clean_text)\n",
    "        # Tokenize & lemmatization\n",
    "        tokens = self.tokenizer(clean_text)\n",
    "        # Remove stop words\n",
    "        if self.remove_stops:\n",
    "            tokens = self._remove_stopwords(tokens)\n",
    "        # Join tokens back into a single string\n",
    "        cleaned_text = \" \".join(tokens)\n",
    "        # self.tokens = tokens\n",
    "        return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78c7d84c-12cc-4a68-a927-a37c615beeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'] = df['Review'].astype(str)\n",
    "\n",
    "# f = TextCleaner(remove_verbs=True)\n",
    "# df['cleaned-reviews'] = df['Review'].map(lambda review: f.clean(review))\n",
    "# df.to_csv(dataset_dir / 'topic_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fcd46da-2dd3-4446-98ab-b4ba18e07e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = TextCleaner(remove_verbs=False)\n",
    "# df['cleaned-reviews'] = df['Review'].map(lambda review: f.clean(review))\n",
    "# df.to_csv(dataset_dir / 'clean_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667228fd-6b0a-438c-8b7f-76796433c5ba",
   "metadata": {},
   "source": [
    "# Custom Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "962216c3-cd0f-4ed6-b964-d088a716b2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>cleaned-reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>5</td>\n",
       "      <td>ambience food lunch cost place sate brunch fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>5</td>\n",
       "      <td>ambience evening experience kudo service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>5</td>\n",
       "      <td>food ambience service recommendation music bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Soumen das and Arun was a great guy. Only beca...</td>\n",
       "      <td>5</td>\n",
       "      <td>guy behavior sincerety food course place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food is good.we ordered Kodi drumsticks and ba...</td>\n",
       "      <td>5</td>\n",
       "      <td>food goodwe drumstick basket mutton biryani thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9882</th>\n",
       "      <td>I am amazed at the quality of food and service...</td>\n",
       "      <td>4</td>\n",
       "      <td>quality food service place ambience location p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>The food was amazing. Do not forget to try 'Mo...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>food sizzler staff chicken town heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9884</th>\n",
       "      <td>We ordered from here via swiggy:\\n\\nWe ordered...</td>\n",
       "      <td>4</td>\n",
       "      <td>swiggy mushroom quantity dish paneer gravy dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9885</th>\n",
       "      <td>I have been to this place on a sunday with my ...</td>\n",
       "      <td>1</td>\n",
       "      <td>place friend meal time friend moment 215pm man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9886</th>\n",
       "      <td>Saturday afternoon we decided to try out this ...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>afternoon ambience place bowl chicken chicken ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9887 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review Rating  \\\n",
       "0     The ambience was good, food was quite good . h...      5   \n",
       "1     Ambience is too good for a pleasant evening. S...      5   \n",
       "2     A must try.. great food great ambience. Thnx f...      5   \n",
       "3     Soumen das and Arun was a great guy. Only beca...      5   \n",
       "4     Food is good.we ordered Kodi drumsticks and ba...      5   \n",
       "...                                                 ...    ...   \n",
       "9882  I am amazed at the quality of food and service...      4   \n",
       "9883  The food was amazing. Do not forget to try 'Mo...    4.5   \n",
       "9884  We ordered from here via swiggy:\\n\\nWe ordered...      4   \n",
       "9885  I have been to this place on a sunday with my ...      1   \n",
       "9886  Saturday afternoon we decided to try out this ...    4.5   \n",
       "\n",
       "                                        cleaned-reviews  \n",
       "0     ambience food lunch cost place sate brunch fri...  \n",
       "1              ambience evening experience kudo service  \n",
       "2     food ambience service recommendation music bac...  \n",
       "3              guy behavior sincerety food course place  \n",
       "4     food goodwe drumstick basket mutton biryani thank  \n",
       "...                                                 ...  \n",
       "9882  quality food service place ambience location p...  \n",
       "9883              food sizzler staff chicken town heart  \n",
       "9884  swiggy mushroom quantity dish paneer gravy dis...  \n",
       "9885  place friend meal time friend moment 215pm man...  \n",
       "9886  afternoon ambience place bowl chicken chicken ...  \n",
       "\n",
       "[9887 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(dataset_dir / 'topic_df.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbe7bdaa-d711-4e15-b5c1-75ec28a3db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'cleaned-reviews'] = df['cleaned-reviews'].astype(str)\n",
    "reviews = df['cleaned-reviews'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2843962f-aac5-4c5f-b8aa-dcd1b17245e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import vocab\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "class CustomVocab(torchtext.vocab.Vocab):\n",
    "    def __init__(self, document: Union[List[str], str]):\n",
    "        super(CustomVocab, self).__init__(None)\n",
    "        \n",
    "        self.rawText = document\n",
    "        self.tokens = []\n",
    "        self.word_freqs = []\n",
    "        self.vocab = self._create_vocab(document)\n",
    "        self.id2words = []\n",
    "        self.bow = self._bag_of_words(document)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def _create_vocab(self, document):\n",
    "        tokens = self._get_tokens(document)\n",
    "        orderedDict = self._get_word_freq(tokens)\n",
    "        \n",
    "        vocab = torchtext.vocab.vocab(ordered_dict=orderedDict, min_freq=1)\n",
    "        vocab.set_default_index(-1)\n",
    "        return vocab\n",
    "\n",
    "    def _get_tokens(self, document):\n",
    "        if isinstance(document, str):\n",
    "            document = [document]\n",
    "\n",
    "        tokens = []\n",
    "        for word in document:\n",
    "            token = word.split(' ')\n",
    "            tokens.extend(token)\n",
    "\n",
    "        self.tokens = tokens\n",
    "        return tokens\n",
    "\n",
    "    def _get_word_freq(self, tokens):       \n",
    "\n",
    "        counter = Counter(tokens)\n",
    "        sort_counter = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.word_freqs = sort_counter\n",
    "        return OrderedDict(counter)\n",
    "\n",
    "    def _bag_of_words(self, document):\n",
    "        words = []\n",
    "        for doc in document:\n",
    "            words.append([token for token in doc.split(' ')])\n",
    "        # print(words)\n",
    "        self.id2words = corpora.Dictionary(words)\n",
    "        \n",
    "        return [self.id2words.doc2bow(word) for word in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d76e93b-2910-49c7-bc69-cc0fa8c1d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = CustomVocab(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b193b296-f83d-474e-b4a0-b78dec957ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vocab.tokens\n",
    "freq = vocab.word_freqs\n",
    "stoi = vocab.get_stoi()\n",
    "bow = vocab.bow\n",
    "id2word = vocab.id2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d093f5cb-8a23-47e3-8e89-8201e7981a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9887, 7819)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow), len(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d98c8-31c2-48ff-8797-0d0208b39281",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "We use an LDA model from gensim library to find relevant topics in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e90c01b-338c-4e14-a89d-94d6b7716c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48d677eb-fe21-4cf6-a8f3-627cc01ffc41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.047*\"place\" + 0.047*\"food\" + 0.020*\"service\" + 0.016*\"time\" + 0.016*\"chicken\" + 0.015*\"one\" + 0.014*\"taste\" + 0.012*\"ambience\" + 0.011*\"restaurant\" + 0.010*\"staff\"'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 1\n",
    "num_cores = os.cpu_count()\n",
    "lda_model = LdaMulticore(corpus=bow, id2word=id2word,\n",
    "                         num_topics=num_topics, iterations=100,\n",
    "                         workers=num_cores)\n",
    "\n",
    "topics = lda_model.print_topics()\n",
    "topics = topics[0][1]\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d9281-bb48-453a-a030-7da8eb55016e",
   "metadata": {},
   "source": [
    "# Topic Extraction using FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05450bf4-dc86-40b6-960f-d61006b96484",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_csv(dataset_dir / 'clean_df.csv')\n",
    "clean_df.loc[:, 'cleaned-reviews'] = clean_df['cleaned-reviews'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2162b726-e03e-4131-a368-1a4b91fd8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_topics = []\n",
    "clean_reviews = clean_df['cleaned-reviews'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95232fdb-55c6-489b-a127-678895495531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "fasttext_model = FastText(clean_reviews, vector_size=100, window=5, min_count=1, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c53dba3-b387-4472-84ae-3eacec15cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b8417-a2a6-488d-8536-61dcafa6fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0 : 'food',\n",
    "    1 : 'service',\n",
    "    2 : 'atmosphare'\n",
    "}\n",
    "\n",
    "clean_df['topic'] = None\n",
    "clean_df = clean_df.drop(columns=['cleaned-reviews'])\n",
    "\n",
    "for row, review in enumerate(clean_reviews):\n",
    "    prob = []\n",
    "    for topic in topics:\n",
    "        prob.append(fasttext_model.wv.n_similarity(review, topic))\n",
    "    clean_df.loc[row, 'topic'] = labels[np.argmax(prob)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7295ed3-d128-4d33-9df0-a6c1e33fde52",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv(dataset_dir / 'clean_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80b6e8-6b4a-481e-8b73-920426e7f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a4f69-abc6-4f20-ae47-0134efff8f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon-kernel",
   "language": "python",
   "name": "hackathon-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
