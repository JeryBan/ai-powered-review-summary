{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4973fb1f-2090-49b4-b2c7-aeea71c091a3",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/JeryBan/ai-powered-review-summary/blob/main/sentiment_analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b800521-aeb8-4a31-8550-f67f147cc977",
   "metadata": {},
   "source": [
    "# Review Classifier based on Sentiment Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5365cbe9-8409-4415-b2ba-5929c6640770",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Dataframe & text preprocess](#Dataframe-&-text-preprocess)\n",
    "- [Setup Configurations](#Setup-Configurations)\n",
    "- [Getting Pretrained Embeddings](#Getting-Pretrained-Embeddings)\n",
    "- [Creating the Model](#Creating-the-Model)\n",
    "- [Creating Datasets and Dataloaders](#Creating-Datasets-and-Dataloaders)\n",
    "- [Training and Evaluation](#Training-and-Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5a6d372f-2f01-4e46-a0a1-021cdb31e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colab\n",
    "# !pip install torchmetrics\n",
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5746bda9-c43e-442c-aa81-bb0ff4ae9a3b",
   "metadata": {},
   "source": [
    "# Dataframe & text preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5ff1f3f-3e8e-4ed3-b3da-38c5b8ee2f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Soumen das and Arun was a great guy. Only beca...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food is good.we ordered Kodi drumsticks and ba...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Madhumathi Mahajan Well to start with nice cou...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>This place has never disappointed us.. The foo...</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Bad rating is mainly because of \"Chicken Bone ...</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>I personally love and prefer Chinese Food. Had...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Checked in here to try some delicious chinese ...</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9955 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review Rating\n",
       "0     The ambience was good, food was quite good . h...      5\n",
       "1     Ambience is too good for a pleasant evening. S...      5\n",
       "2     A must try.. great food great ambience. Thnx f...      5\n",
       "3     Soumen das and Arun was a great guy. Only beca...      5\n",
       "4     Food is good.we ordered Kodi drumsticks and ba...      5\n",
       "...                                                 ...    ...\n",
       "9995  Madhumathi Mahajan Well to start with nice cou...      3\n",
       "9996  This place has never disappointed us.. The foo...    4.5\n",
       "9997  Bad rating is mainly because of \"Chicken Bone ...    1.5\n",
       "9998  I personally love and prefer Chinese Food. Had...      4\n",
       "9999  Checked in here to try some delicious chinese ...    3.5\n",
       "\n",
       "[9955 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_dir = Path('./data')\n",
    "csv_dir = dataset_dir / 'raw' / 'Restaurant reviews.csv'\n",
    "\n",
    "df = pd.read_csv(csv_dir, usecols=['Review', 'Rating']).dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81a0ca82-ee49-4210-b13c-22e8395079f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.df_setup import clean_dataframe\n",
    "\n",
    "df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ac638a-9f40-44a1-be14-0337a2937335",
   "metadata": {},
   "source": [
    "* turn ratings to binary labels for the sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb61a325-7033-45f1-bdcc-cea354793f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'Rating'] = df['Rating'].astype(float)\n",
    "df.loc[:, 'Rating'] = df['Rating'].map(lambda rating: 1 if rating > 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "71184630-91a5-4f7e-96ae-61dfd6da10d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates:\n",
      "0\n",
      "\n",
      "Missing indexes:\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates or missing values\n",
    "duplicate_index = df.index[df.index.duplicated()]\n",
    "print('Duplicates:')\n",
    "print(len(duplicate_index))\n",
    "\n",
    "print('\\nMissing indexes:')\n",
    "missing_index = set(range(len(df))) - set(df.index)\n",
    "print(len(missing_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78dfd17e-8d85-4b78-b3fe-1318c0781d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing indexes:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df = df.reindex(range(len(df)))\n",
    "missing_index = set(range(len(df))) - set(df.index)\n",
    "df = df.dropna()\n",
    "print('Missing indexes:')\n",
    "print(len(missing_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2449aa05-ad8a-4897-8158-404c3e425f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean reviews but keep the verbs to help in sentiment analysis\n",
    "from src.utils.text_setup import TextCleaner\n",
    "\n",
    "f = TextCleaner(remove_verbs=False)\n",
    "# df['cleaned-reviews'] = df['Review'].map(lambda review: f.clean(review))\n",
    "# df.to_csv(dataset_dir / 'processed' /'clean_with_verbs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f626e871-0683-4613-b68f-d88988bdce33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>cleaned-reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambience good food quite good saturday lunch c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambience good pleasant evening service prompt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>1</td>\n",
       "      <td>must try great food great ambience thnx servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Soumen das and Arun was a great guy. Only beca...</td>\n",
       "      <td>1</td>\n",
       "      <td>soumen das arun great guy behavior sincerety g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food is good.we ordered Kodi drumsticks and ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>food good.we order kodi drumstick basket mutto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9881</th>\n",
       "      <td>Been looking for Chinese food around gachibowl...</td>\n",
       "      <td>1</td>\n",
       "      <td>look chinese food around gachibowli find place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9882</th>\n",
       "      <td>I am amazed at the quality of food and service...</td>\n",
       "      <td>1</td>\n",
       "      <td>amazed quality food service place provide opul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>The food was amazing. Do not forget to try 'Mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>food amazing forget try mou chi kay amazing si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9884</th>\n",
       "      <td>We ordered from here via swiggy:\\n\\nWe ordered...</td>\n",
       "      <td>1</td>\n",
       "      <td>order via swiggy order stuff mushroom little s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9885</th>\n",
       "      <td>I have been to this place on a sunday with my ...</td>\n",
       "      <td>0</td>\n",
       "      <td>place sunday friend think good meal spend time...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9886 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review  Rating  \\\n",
       "0     The ambience was good, food was quite good . h...       1   \n",
       "1     Ambience is too good for a pleasant evening. S...       1   \n",
       "2     A must try.. great food great ambience. Thnx f...       1   \n",
       "3     Soumen das and Arun was a great guy. Only beca...       1   \n",
       "4     Food is good.we ordered Kodi drumsticks and ba...       1   \n",
       "...                                                 ...     ...   \n",
       "9881  Been looking for Chinese food around gachibowl...       1   \n",
       "9882  I am amazed at the quality of food and service...       1   \n",
       "9883  The food was amazing. Do not forget to try 'Mo...       1   \n",
       "9884  We ordered from here via swiggy:\\n\\nWe ordered...       1   \n",
       "9885  I have been to this place on a sunday with my ...       0   \n",
       "\n",
       "                                        cleaned-reviews  \n",
       "0     ambience good food quite good saturday lunch c...  \n",
       "1     ambience good pleasant evening service prompt ...  \n",
       "2     must try great food great ambience thnx servic...  \n",
       "3     soumen das arun great guy behavior sincerety g...  \n",
       "4     food good.we order kodi drumstick basket mutto...  \n",
       "...                                                 ...  \n",
       "9881  look chinese food around gachibowli find place...  \n",
       "9882  amazed quality food service place provide opul...  \n",
       "9883  food amazing forget try mou chi kay amazing si...  \n",
       "9884  order via swiggy order stuff mushroom little s...  \n",
       "9885  place sunday friend think good meal spend time...  \n",
       "\n",
       "[9886 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/processed/clean_with_verbs.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0dba32-7395-46f9-8d83-3c84cafc87f3",
   "metadata": {},
   "source": [
    "# Setup Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "496e3eea-f846-4d15-9846-c80ac4966317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torchinfo import summary\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4be1d184-b530-44f2-81d9-fc359087ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conf:\n",
    "    # training\n",
    "    EPOCHS = 1\n",
    "    LOSS_FN = nn.BCELoss()\n",
    "    LR = 0.001\n",
    "    BATCH_SIZE = 32\n",
    "    # metrics\n",
    "    ACC_FN = Accuracy(task='binary').to(device)\n",
    "    F1 = F1Score(task='binary').to(device)\n",
    "    # model\n",
    "    EMBEDDING_DIM = 200\n",
    "    HIDDEN_UNITS = 128\n",
    "    ATTN_HEADS = 1\n",
    "    LSTM_LAYERS = 2\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7875c36-3e32-49e0-aa17-ef1f9443651d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ambience good food quite good saturday lunch cost effective good place sate brunch one also chill friend parent waiter soumen das really courteous helpful',\n",
       " 'ambience good pleasant evening service prompt food good good experience soumen das kudo service']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned-reviews'] = df['cleaned-reviews'].astype(str)\n",
    "df['Rating'] = df['Rating'].astype(int)\n",
    "reviews = df['cleaned-reviews'].values.tolist()\n",
    "labels = df['Rating'].values.tolist()\n",
    "reviews[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec636729-8e69-47a3-8cd4-7c67f1ef5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.vocab import create_vocabulary\n",
    "\n",
    "vocab = create_vocabulary(reviews, min_freq=3)\n",
    "vocab.save('./data/saved_models/sentiment_vocab.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984c852-3511-4a63-8851-55c110afbea7",
   "metadata": {},
   "source": [
    "# Getting Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c261fa74-14ce-44ba-acc2-82d1fbe04b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile src/utils/embeddings.py\n",
    "\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_weights():\n",
    "    '''Downloads glove.6B.200d.txt , pretrained word weights\n",
    "       and returns the path of the downloaded file.'''\n",
    "    file_destination = Path('data/saved_models')\n",
    "    \n",
    "    print('Downloading weights...')\n",
    "    with open('glove.6B.200d.zip', 'wb') as f:\n",
    "        response = requests.get('https://storage.googleapis.com/kaggle-data-sets/13926/18767/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240229%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240229T114040Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=4c1c59d7ff03b749bc5a3d74c5211a9dc203f9b3d4582ec867eb582e4ed7725d935ccb77dc1b7dc5b0216245492c765b0091c3519e266f750447a56cd34b5fa8c6de769c722c1ad37ddd891a9926ee87d7823d889da35922efbe07bfc4abf9991680951b4a246d3b3fc0b2ca7e2b6524d03f85a5718c2d04a0bf45150eb504707e64230862fb1439ea5ffab680cdff33c50b493e625702b4f8d594d2bc6a1c7116f4b63edf2fbb65eef09510dc9b9687997bfb331cdb414540b758db3d904d33dd129a09f41ece8e9223138fe6bff3ff62a77b8737dee24ff2175b37593121500445b822d863bbf2919cfcb1885947ab24e3a14d3afdcba79a211569379d9d44')\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f'Unziping to {file_destination} ...')\n",
    "    with zipfile.ZipFile('glove.6B.200d.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(file_destination)\n",
    "\n",
    "    os.remove('glove.6B.200d.zip')\n",
    "    print('Done')\n",
    "\n",
    "    return file_destination / 'glove.6B.200d.txt'\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_embedding_matrix(weights_path):\n",
    "    '''Populates a matrix with the pretrained weights for every word\n",
    "       in the vocabulary present in the weights file.'''\n",
    "    vocab = Dictionary.load('./data/saved_models/sentiment_vocab.pt')\n",
    "    stoi = vocab.token2id\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    \n",
    "    f = open(weights_path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(stoi) + 1, 200))\n",
    "    \n",
    "    for word, i in stoi.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9142fda-6ee4-42c0-a5be-f47c4e8417f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = Path('data/saved_models/glove.6B.200d.txt') #download_weights()\n",
    "\n",
    "embedding_matrix = get_embedding_matrix(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b6345f1-d4d5-43c9-a1b3-33a31ec15b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4944, 200])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dfc7d6-7e38-4e88-aed3-9e5f6568a519",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6760efa6-0b76-4bb5-9adf-86b299154209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile src/models/lstm_model.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim = embedding_dim,\n",
    "                                          num_heads = num_heads,\n",
    "                                          batch_first = True)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        output, _ = self.attn(embeddings, embeddings, embeddings)\n",
    "        return output\n",
    "        \n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embedding_dim, hidden_units, lstm_layers: int, attn_heads: int):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    " \n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings = embedding_matrix, freeze = True)\n",
    "\n",
    "        self.attn_layer = SelfAttention(embedding_dim = embedding_dim,\n",
    "                                        num_heads = attn_heads)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_units, \n",
    "                            num_layers = lstm_layers, \n",
    "                            batch_first = True, \n",
    "                            bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Linear(in_features = hidden_units * 2, out_features = hidden_units),\n",
    "                                nn.Linear(in_features = hidden_units, out_features = 1),\n",
    "                                nn.Dropout(0.2))\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        embeddings = self.embeddings(x)\n",
    "\n",
    "        attn_out = self.attn_layer(embeddings)\n",
    "    \n",
    "        lstm_out, _ = self.lstm(attn_out)\n",
    "    \n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        fc_out = self.fc(lstm_out).squeeze(1)\n",
    "\n",
    "        output = self.sigmoid(fc_out)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a354601a-d651-4db5-9d55-2b182af4aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(embedding_matrix=embedding_matrix,\n",
    "                       embedding_dim=conf.EMBEDDING_DIM, \n",
    "                       hidden_units=conf.HIDDEN_UNITS,\n",
    "                       lstm_layers=conf.LSTM_LAYERS,\n",
    "                       attn_heads=conf.ATTN_HEADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e340a55-8295-4f9e-aca3-35dd0f865cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "LSTMClassifier                           [1, 200]                  [1]                       --\n",
       "├─Embedding: 1-1                         [1, 200]                  [1, 200, 200]             (988,800)\n",
       "├─SelfAttention: 1-2                     [1, 200, 200]             [1, 200, 200]             --\n",
       "│    └─MultiheadAttention: 2-1           [1, 200, 200]             [1, 200, 200]             160,800\n",
       "├─LSTM: 1-3                              [1, 200, 200]             [1, 200, 256]             733,184\n",
       "├─Sequential: 1-4                        [1, 256]                  [1, 1]                    --\n",
       "│    └─Linear: 2-2                       [1, 256]                  [1, 128]                  32,896\n",
       "│    └─Linear: 2-3                       [1, 128]                  [1, 1]                    129\n",
       "│    └─Dropout: 2-4                      [1, 1]                    [1, 1]                    --\n",
       "├─Sigmoid: 1-5                           [1]                       [1]                       --\n",
       "===================================================================================================================\n",
       "Total params: 1,915,809\n",
       "Trainable params: 927,009\n",
       "Non-trainable params: 988,800\n",
       "Total mult-adds (Units.MEGABYTES): 147.66\n",
       "===================================================================================================================\n",
       "Input size (MB): 11.07\n",
       "Forward/backward pass size (MB): 0.73\n",
       "Params size (MB): 7.02\n",
       "Estimated Total Size (MB): 18.82\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model,\n",
    "        input_data=train_dataset[0][0].unsqueeze(0),\n",
    "        col_names=['input_size', 'output_size', 'num_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ecc08-48a2-435b-9b0c-1c6925b363a6",
   "metadata": {},
   "source": [
    "# Creating Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2cac9fd-0668-46ee-b359-02d50a9e4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile src/utils/data.py\n",
    "\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchtext.functional import to_tensor\n",
    "import torchtext.transforms as T\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    '''Creates a torch.utils.data.Dataset and applies given transformations.'''\n",
    "    def __init__(self, data: List[str], labels: list):\n",
    "        super().__init__()\n",
    "\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "        self.data = custom_transforms(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "        \n",
    "\n",
    "def train_test_split(data: List[str], labels: list, split_point: float = 0.7):\n",
    "    '''Splits the data to train and test segments according to split_point.'''\n",
    "    split = int(split_point * len(data))\n",
    "    \n",
    "    train_data = data[0 : split]\n",
    "    train_labels = labels[0 : split]\n",
    "\n",
    "    test_data = data[split : len(data) -1]\n",
    "    test_labels = labels[split : len(labels) -1]\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def create_dataloaders(train_dataset, test_dataset, batch_size: int):\n",
    "    '''Creates train and test dataloaders of given batch size.'''\n",
    "    train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_workers=os.cpu_count(),\n",
    "                                  shuffle=False)\n",
    "\n",
    "    test_dataloader = DataLoader(dataset=test_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_workers=os.cpu_count(),\n",
    "                                  shuffle=False)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "def custom_transforms(data: List[str], max_seq_len: int = 200) -> torch.Tensor:\n",
    "    '''Converts words to ids truncates and returns sentences as tensors.'''\n",
    "    vocab = Dictionary.load('./data/saved_models/sentiment_vocab.pt')\n",
    "    \n",
    "    f = T.Truncate(max_seq_len=max_seq_len)\n",
    "    \n",
    "    sent2ids = [vocab.doc2idx(sentence.split(' '), unknown_word_index=1) for sentence in data]\n",
    "    sent2ids = f(sent2ids)\n",
    "    \n",
    "    return to_tensor(sent2ids, padding_value=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0dec8088-7444-4d82-916c-08eb5d06702e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 2965)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, train_labels, test_data, test_labels = train_test_split(reviews, labels)\n",
    "\n",
    "train_dataset = CustomDataset(data=train_data, labels=train_labels)\n",
    "test_dataset = CustomDataset(data=test_data, labels=test_labels)\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58c91365-7171-4298-aa4e-a9fb802ad237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217, 93)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader, test_dataloader = create_dataloaders(train_dataset, test_dataset, conf.BATCH_SIZE)\n",
    "\n",
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa8431-9a99-4930-9b41-605848b6de8b",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5c6978e-07f6-435f-b1a5-bc7c6cdea67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile src/training/training.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def train_step(model: nn.Module,\n",
    "               loss_fn: nn.Module,\n",
    "               optimizer: torch.optim,\n",
    "               acc_fn,\n",
    "               dataloader: torch.utils.data,\n",
    "               device: torch.device):\n",
    "    '''Training step during model training'''\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        logits = model(X)\n",
    "        preds = torch.round(logits)\n",
    "        \n",
    "        loss = loss_fn(logits, y)\n",
    "        acc = acc_fn(preds, y)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "\n",
    "\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "\n",
    "def test_step(model: nn.Module,\n",
    "              loss_fn: nn.Module,\n",
    "              acc_fn,\n",
    "              f1,\n",
    "              dataloader: torch.utils.data,\n",
    "              device: torch.device):\n",
    "    '''Test step during evaluation'''\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    test_loss, test_acc, f1_score = 0, 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            test_logits = model(X)\n",
    "            test_preds = torch.round(test_logits)\n",
    "            \n",
    "            loss = loss_fn(test_logits, y).item()\n",
    "            acc = acc_fn(test_preds, y).item()\n",
    "            score = f1(test_preds, y).item()\n",
    "\n",
    "            test_loss += loss\n",
    "            test_acc += acc\n",
    "            f1_score += score\n",
    "\n",
    "        test_loss /= len(dataloader)\n",
    "        test_acc /= len(dataloader)\n",
    "        f1_score /= len(dataloader)\n",
    "\n",
    "    return test_loss, test_acc, f1_score\n",
    "\n",
    "\n",
    "\n",
    "def train(model: nn.Module,\n",
    "         loss_fn: nn.Module,\n",
    "         optimizer: torch.optim,\n",
    "         train_dataloader: torch.utils.data,\n",
    "         test_dataloader: torch.utils.data,\n",
    "         acc_fn,\n",
    "         f1,\n",
    "         epochs: int,\n",
    "         device: torch.device):\n",
    "\n",
    "    results = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc': [],\n",
    "        'f1_score': []\n",
    "    }\n",
    "\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch: {epoch}\\n--------')\n",
    "\n",
    "        train_loss, train_acc = train_step(model=model, loss_fn=loss_fn, optimizer=optimizer, acc_fn=acc_fn, dataloader=train_dataloader, device=device)\n",
    "        results['train_loss'].append(train_loss)\n",
    "        results['train_acc'].append(train_acc)\n",
    "\n",
    "        print()\n",
    "        \n",
    "        test_loss, test_acc, f1_score = test_step(model=model, loss_fn=loss_fn, acc_fn=acc_fn, f1=f1, dataloader=test_dataloader, device=device)\n",
    "        results['test_loss'].append(test_loss)\n",
    "        results['test_acc'].append(test_acc)\n",
    "        results['f1_score'].append(f1_score)\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "                best_test_loss = test_loss\n",
    "                torch.save(model.state_dict(), f'lstm-{epochs}epochs-{conf.LSTM_LAYERS}lstm-{conf.ATTN_HEADS}attn-{conf.BATCH_SIZE}.pth')\n",
    "                print('saved')\n",
    "\n",
    "        print(f'train loss: {train_loss:.4f} | train acc: {train_acc:.2f}%')\n",
    "        print(f'test loss: {test_loss:.4f} | test acc: {test_acc:.2f}%')\n",
    "        print(f'f1 score: {f1_score:.2f}')\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "37e27c1f-ffbe-48de-b154-6b027d760810",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(embedding_matrix=embedding_matrix, \n",
    "                       embedding_dim=conf.EMBEDDING_DIM, \n",
    "                       hidden_units=conf.HIDDEN_UNITS,\n",
    "                       lstm_layers=conf.LSTM_LAYERS,\n",
    "                       attn_heads=conf.ATTN_HEADS)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=conf.LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd09fad-f47a-48c3-b7f7-b7f8e29ed019",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(model=model,\n",
    "               loss_fn=conf.LOSS_FN,\n",
    "               optimizer=optimizer,\n",
    "               train_dataloader=train_dataloader,\n",
    "               test_dataloader=test_dataloader,\n",
    "               acc_fn=conf.ACC_FN,\n",
    "               f1=conf.F1,\n",
    "               epochs=conf.EPOCHS,\n",
    "               device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "063601d1-6598-4e9b-91e5-09e144b815ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile src/evaluation/plot.py\n",
    "\n",
    "from typing import Dict, List\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curves(results: Dict[str, List[float]]):\n",
    "    \"\"\"Plots training curves of a results dictionary.\"\"\"\n",
    "    \n",
    "    loss = results['train_loss']\n",
    "    test_loss = results['test_loss']\n",
    "\n",
    "    accuracy = results['train_acc']\n",
    "    test_accuracy = results['test_acc']\n",
    "\n",
    "    f1_score = results['f1_score']\n",
    "\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, loss, label='train_loss')\n",
    "    plt.plot(epochs, test_loss, label='test_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # accuracy\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
    "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend();\n",
    "\n",
    "    # f1 score\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, f1_score, label='f1_score')\n",
    "    plt.title('F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a7507-886a-4631-87e1-95cae2a8fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51714a0-bc7b-4c77-881e-eeab283a4c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon-kernel",
   "language": "python",
   "name": "hackathon-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
