{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4973fb1f-2090-49b4-b2c7-aeea71c091a3",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/ntl2222/HackathonAI/blob/nikos/sentiment_classifier.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b800521-aeb8-4a31-8550-f67f147cc977",
   "metadata": {},
   "source": [
    "# Review Classifier based on Sentiment Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5365cbe9-8409-4415-b2ba-5929c6640770",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Dataframe & text preprocess](#Dataframe-&-text-preprocess)\n",
    "- [Setup Configurations](#Setup-Configurations)\n",
    "- [Creating the Model](#Creating-the-Model)\n",
    "- [Creating Datasets and Dataloaders](#Creating-Datasets-and-Dataloaders)\n",
    "- [Training](#Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "5a6d372f-2f01-4e46-a0a1-021cdb31e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colab\n",
    "# !pip install torchinfo\n",
    "# !pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5746bda9-c43e-442c-aa81-bb0ff4ae9a3b",
   "metadata": {},
   "source": [
    "# Dataframe & text preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5ff1f3f-3e8e-4ed3-b3da-38c5b8ee2f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Soumen das and Arun was a great guy. Only beca...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food is good.we ordered Kodi drumsticks and ba...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Madhumathi Mahajan Well to start with nice cou...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>This place has never disappointed us.. The foo...</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Bad rating is mainly because of \"Chicken Bone ...</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>I personally love and prefer Chinese Food. Had...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Checked in here to try some delicious chinese ...</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9955 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review Rating\n",
       "0     The ambience was good, food was quite good . h...      5\n",
       "1     Ambience is too good for a pleasant evening. S...      5\n",
       "2     A must try.. great food great ambience. Thnx f...      5\n",
       "3     Soumen das and Arun was a great guy. Only beca...      5\n",
       "4     Food is good.we ordered Kodi drumsticks and ba...      5\n",
       "...                                                 ...    ...\n",
       "9995  Madhumathi Mahajan Well to start with nice cou...      3\n",
       "9996  This place has never disappointed us.. The foo...    4.5\n",
       "9997  Bad rating is mainly because of \"Chicken Bone ...    1.5\n",
       "9998  I personally love and prefer Chinese Food. Had...      4\n",
       "9999  Checked in here to try some delicious chinese ...    3.5\n",
       "\n",
       "[9955 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_dir = Path('./data')\n",
    "csv_dir = dataset_dir / 'raw' / 'Restaurant reviews.csv'\n",
    "\n",
    "df = pd.read_csv(csv_dir, usecols=['Review', 'Rating']).dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81a0ca82-ee49-4210-b13c-22e8395079f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.df_setup import clean_dataframe\n",
    "\n",
    "df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ac638a-9f40-44a1-be14-0337a2937335",
   "metadata": {},
   "source": [
    "* turn ratings to binary labels for the sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb61a325-7033-45f1-bdcc-cea354793f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'Rating'] = df['Rating'].astype(float)\n",
    "df.loc[:, 'Rating'] = df['Rating'].map(lambda rating: 1 if rating > 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "71184630-91a5-4f7e-96ae-61dfd6da10d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates:\n",
      "0\n",
      "\n",
      "Missing indexes:\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates or missing values\n",
    "duplicate_index = df.index[df.index.duplicated()]\n",
    "print('Duplicates:')\n",
    "print(len(duplicate_index))\n",
    "\n",
    "print('\\nMissing indexes:')\n",
    "missing_index = set(range(len(df))) - set(df.index)\n",
    "print(len(missing_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78dfd17e-8d85-4b78-b3fe-1318c0781d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing indexes:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df = df.reindex(range(len(df)))\n",
    "missing_index = set(range(len(df))) - set(df.index)\n",
    "df = df.dropna()\n",
    "print('Missing indexes:')\n",
    "print(len(missing_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2449aa05-ad8a-4897-8158-404c3e425f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean reviews but keep the verbs to help in sentiment analysis\n",
    "from src.utils.text_setup import TextCleaner\n",
    "\n",
    "f = TextCleaner(remove_verbs=False)\n",
    "# df['cleaned-reviews'] = df['Review'].map(lambda review: f.clean(review))\n",
    "# df.to_csv(dataset_dir / 'processed' /'clean_with_verbs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f626e871-0683-4613-b68f-d88988bdce33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>cleaned-reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambience good food quite good saturday lunch c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambience good pleasant evening service prompt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>1</td>\n",
       "      <td>must try great food great ambience thnx servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Soumen das and Arun was a great guy. Only beca...</td>\n",
       "      <td>1</td>\n",
       "      <td>soumen das arun great guy behavior sincerety g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food is good.we ordered Kodi drumsticks and ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>food good.we order kodi drumstick basket mutto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9881</th>\n",
       "      <td>Been looking for Chinese food around gachibowl...</td>\n",
       "      <td>1</td>\n",
       "      <td>look chinese food around gachibowli find place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9882</th>\n",
       "      <td>I am amazed at the quality of food and service...</td>\n",
       "      <td>1</td>\n",
       "      <td>amazed quality food service place provide opul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>The food was amazing. Do not forget to try 'Mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>food amazing forget try mou chi kay amazing si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9884</th>\n",
       "      <td>We ordered from here via swiggy:\\n\\nWe ordered...</td>\n",
       "      <td>1</td>\n",
       "      <td>order via swiggy order stuff mushroom little s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9885</th>\n",
       "      <td>I have been to this place on a sunday with my ...</td>\n",
       "      <td>0</td>\n",
       "      <td>place sunday friend think good meal spend time...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9886 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review  Rating  \\\n",
       "0     The ambience was good, food was quite good . h...       1   \n",
       "1     Ambience is too good for a pleasant evening. S...       1   \n",
       "2     A must try.. great food great ambience. Thnx f...       1   \n",
       "3     Soumen das and Arun was a great guy. Only beca...       1   \n",
       "4     Food is good.we ordered Kodi drumsticks and ba...       1   \n",
       "...                                                 ...     ...   \n",
       "9881  Been looking for Chinese food around gachibowl...       1   \n",
       "9882  I am amazed at the quality of food and service...       1   \n",
       "9883  The food was amazing. Do not forget to try 'Mo...       1   \n",
       "9884  We ordered from here via swiggy:\\n\\nWe ordered...       1   \n",
       "9885  I have been to this place on a sunday with my ...       0   \n",
       "\n",
       "                                        cleaned-reviews  \n",
       "0     ambience good food quite good saturday lunch c...  \n",
       "1     ambience good pleasant evening service prompt ...  \n",
       "2     must try great food great ambience thnx servic...  \n",
       "3     soumen das arun great guy behavior sincerety g...  \n",
       "4     food good.we order kodi drumstick basket mutto...  \n",
       "...                                                 ...  \n",
       "9881  look chinese food around gachibowli find place...  \n",
       "9882  amazed quality food service place provide opul...  \n",
       "9883  food amazing forget try mou chi kay amazing si...  \n",
       "9884  order via swiggy order stuff mushroom little s...  \n",
       "9885  place sunday friend think good meal spend time...  \n",
       "\n",
       "[9886 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/processed/clean_with_verbs.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0dba32-7395-46f9-8d83-3c84cafc87f3",
   "metadata": {},
   "source": [
    "# Setup Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "496e3eea-f846-4d15-9846-c80ac4966317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4be1d184-b530-44f2-81d9-fc359087ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conf:\n",
    "    # training\n",
    "    EPOCHS = 1\n",
    "    LOSS_FN = nn.CrossEntropyLoss()\n",
    "    LR = 0.001\n",
    "    BATCH_SIZE = 32\n",
    "    ACC_FN = Accuracy(task='binary').to(device)\n",
    "    # roberta\n",
    "    EMBEDDING_DIM = 120\n",
    "    FFN_DIMENSION = 150\n",
    "    ATT_HEADS = 3\n",
    "    ENCODERS = 2\n",
    "    DROPOUT = 0.4\n",
    "    # classifier\n",
    "    HIDDEN_UNITS = 50\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dfc7d6-7e38-4e88-aed3-9e5f6568a519",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7875c36-3e32-49e0-aa17-ef1f9443651d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ambience good food quite good saturday lunch cost effective good place sate brunch one also chill friend parent waiter soumen das really courteous helpful',\n",
       " 'ambience good pleasant evening service prompt food good good experience soumen das kudo service']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned-reviews'] = df['cleaned-reviews'].astype(str)\n",
    "df['Rating'] = df['Rating'].astype(int)\n",
    "reviews = df['cleaned-reviews'].values.tolist()\n",
    "labels = df['Rating'].values.tolist()\n",
    "reviews[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec636729-8e69-47a3-8cd4-7c67f1ef5368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14486"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils.vocab_setup import CustomVocab\n",
    "\n",
    "vocab = CustomVocab(reviews)\n",
    "vocab_size = vocab.size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58afe050-6ff3-49f5-886c-6fa5b97c999c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3656"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = 0\n",
    "\n",
    "for review in reviews:\n",
    "    if len(review) > max_seq_length: max_seq_length = len(review)\n",
    "\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "48171c0f-cd07-4072-8da4-33951893edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile /src/models/roberta_model.py\n",
    "\n",
    "import torchtext\n",
    "\n",
    "def model_init(vocab_size: int,\n",
    "                 embedding_dim: int,\n",
    "                 ffn_dimension: int,\n",
    "                 num_attention_heads: int,\n",
    "                 num_encoder_layers: int,\n",
    "                 max_seq_len: int,\n",
    "                 padding_idx: int = 1,\n",
    "                 dropout: float = 0.1,\n",
    "                 scaling = None,\n",
    "                 normalize_before: bool = False):\n",
    "    '''Initialize a distilled RoBERTa encoder and returns the\n",
    "       model and the transformations it was trained.'''\n",
    "\n",
    "    base = torchtext.models.ROBERTA_DISTILLED_ENCODER\n",
    "    config = base.encoderConf\n",
    "\n",
    "    config.vocab_size=vocab_size\n",
    "    config.embedding_dim=embedding_dim\n",
    "    config.ffn_dimension=ffn_dimension\n",
    "    config.padding_idx=padding_idx\n",
    "    config.max_seq_len=max_seq_len\n",
    "    config.num_attention_heads=num_attention_heads\n",
    "    config.num_encoder_layers=num_encoder_layers\n",
    "    config.dropout=dropout\n",
    "    config.scaling=scaling\n",
    "    config.normalize_before=normalize_before\n",
    "\n",
    "    model = base.build_model(encoder_conf=config)\n",
    "    transforms = base.transform()\n",
    "\n",
    "    transformer_encoder = model.encoder.transformer.layers.layers\n",
    "\n",
    "    for i in range(num_encoder_layers):\n",
    "        transformer_encoder.get_submodule(f'{i}').linear1 = nn.Linear(in_features=embedding_dim, out_features=ffn_dimension, bias=True)\n",
    "        transformer_encoder.get_submodule(f'{i}').linear2 = nn.Linear(in_features=ffn_dimension, out_features=embedding_dim, bias=True)\n",
    "    \n",
    "    print(config)\n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d3e5e981-be9b-420b-97e4-e32e8d51cc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerybananas/Documents/python/_venvs/hackathon-venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaEncoderConf(vocab_size=50265, embedding_dim=120, ffn_dimension=150, padding_idx=1, max_seq_len=3656, num_attention_heads=3, num_encoder_layers=2, dropout=0.4, scaling=None, normalize_before=False)\n"
     ]
    }
   ],
   "source": [
    "roberta, transforms = model_init(vocab_size = 50265,\n",
    "                                 embedding_dim = conf.EMBEDDING_DIM,\n",
    "                                 ffn_dimension = conf.FFN_DIMENSION,\n",
    "                                 num_attention_heads = conf.ATT_HEADS,\n",
    "                                 num_encoder_layers = conf.ENCODERS,\n",
    "                                 dropout=conf.DROPOUT,\n",
    "                                 max_seq_len = max_seq_length)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9f63c61f-4434-4130-9e6a-2b4da7e25149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): GPT2BPETokenizer()\n",
      "  (1): VocabTransform(\n",
      "    (vocab): Vocab()\n",
      "  )\n",
      "  (2): Truncate()\n",
      "  (3): AddToken()\n",
      "  (4): AddToken()\n",
      ") \n",
      "\n",
      "RobertaModel(\n",
      "  (encoder): RobertaEncoder(\n",
      "    (transformer): TransformerEncoder(\n",
      "      (token_embedding): Embedding(50265, 120, padding_idx=1)\n",
      "      (layers): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=120, out_features=150, bias=True)\n",
      "            (dropout): Dropout(p=0.4, inplace=False)\n",
      "            (linear2): Linear(in_features=150, out_features=120, bias=True)\n",
      "            (norm1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.4, inplace=False)\n",
      "            (dropout2): Dropout(p=0.4, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (positional_embedding): PositionalEmbedding(\n",
      "        (embedding): Embedding(3656, 120, padding_idx=1)\n",
      "      )\n",
      "      (embedding_layer_norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.4, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(transforms, '\\n')\n",
    "print(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5509e127-c6a8-4fad-bef6-3a4dba8ddbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  3146, 11465,   205,   689,  1341,   205,   579, 47035,  4592,\n",
      "           701,  2375,   205,   317,   579,   877, 25003,    65,    67, 13146,\n",
      "          1441,  4095, 38233, 22272,  2262,   385,   281,   269, 17867,   859,\n",
      "          1827,  7163,     2],\n",
      "        [    0,  3146, 11465,   205, 16219,  1559,   544, 14302,   689,   205,\n",
      "           205,   676, 22272,  2262,   385,   281,   449, 23259,   544,     2,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1]]) \n",
      "\n",
      "torch.Size([2, 33])\n"
     ]
    }
   ],
   "source": [
    "from torchtext.functional import to_tensor\n",
    "\n",
    "transformed_reviews = to_tensor(transforms(reviews[:2]), padding_value=1)\n",
    "\n",
    "print(transformed_reviews, '\\n')\n",
    "print(transformed_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e6d0594-8d24-4f2c-844d-689bb1ad9886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 33, 120])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = roberta(transformed_reviews)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fed245e7-3701-4e61-93f6-e717c3d0cfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, embedding_layer, embedding_dim, hidden_units):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.classifier = nn.Sequential(nn.Linear(in_features=embedding_dim, out_features=hidden_units),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(in_features=hidden_units, out_features=2))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # get token embeddings from a pretrained model\n",
    "        embeddings = self.embedding_layer(x)\n",
    "\n",
    "        # Pooling to get a single vector representation for each sequence in the batch\n",
    "        pooling = torch.mean(embeddings, dim=1)\n",
    "\n",
    "        logits = self.classifier(pooling)\n",
    "    \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c79424a0-ec93-447e-af7b-1a431f921880",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentClassifier(embedding_layer=roberta, embedding_dim=conf.EMBEDDING_DIM, hidden_units=conf.HIDDEN_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1a1f875-b17c-4ac3-ab1c-4d8ca6796aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities:\n",
      "tensor([[0.4717, 0.5283],\n",
      "        [0.4578, 0.5422]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "predictions:\n",
      "tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y = model(transformed_reviews)\n",
    "probabilities = torch.softmax(y, dim=-1)\n",
    "predictions = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "print(f'probabilities:\\n{probabilities}\\n')\n",
    "print(f'predictions:\\n{predictions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ecc08-48a2-435b-9b0c-1c6925b363a6",
   "metadata": {},
   "source": [
    "# Creating Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f2cac9fd-0668-46ee-b359-02d50a9e4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile src/utils/data_setup.py\n",
    "\n",
    "from typing import List, Optional\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.functional import to_tensor\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    '''Creates a torch.utils.data.Dataset and applies given transformations.'''\n",
    "    def __init__(self, data: List[str], labels: list, transforms: Optional = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "        if transforms:\n",
    "            self.data = to_tensor(self.transforms(data), padding_value=1)\n",
    "        else:\n",
    "            self.data = to_tensor(data, padding_value=1)\n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "        \n",
    "\n",
    "def train_test_split(data: List[str], labels: list, split_point: float = 0.7):\n",
    "    '''Splits the data to train and test segments according to split_point.'''\n",
    "    split = int(split_point * len(data))\n",
    "    \n",
    "    train_data = data[0 : split]\n",
    "    train_labels = labels[0 : split]\n",
    "\n",
    "    test_data = data[split : len(data) -1]\n",
    "    test_labels = labels[split : len(labels) -1]\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def create_dataloaders(train_dataset, test_dataset, batch_size: int):\n",
    "    '''Creates train and test dataloaders of given batch size.'''\n",
    "    train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_workers=os.cpu_count(),\n",
    "                                  shuffle=True)\n",
    "\n",
    "    test_dataloader = DataLoader(dataset=test_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_workers=os.cpu_count(),\n",
    "                                  shuffle=False)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0dec8088-7444-4d82-916c-08eb5d06702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels = train_test_split(reviews, labels)\n",
    "\n",
    "train_dataset = CustomDataset(data=train_data, labels=train_labels, transforms=transforms)\n",
    "test_dataset = CustomDataset(data=test_data, labels=test_labels, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c98b3d21-c2f3-4290-80ec-a7820d5980e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7908, 1977)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58c91365-7171-4298-aa4e-a9fb802ad237",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = create_dataloaders(train_dataset, test_dataset, conf.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa8431-9a99-4930-9b41-605848b6de8b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abd87761-e29a-4adb-9c14-74c13a7f42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: nn.Module,\n",
    "               loss_fn: nn.Module,\n",
    "               optimizer: torch.optim,\n",
    "               acc_fn,\n",
    "               dataloader: torch.utils.data,\n",
    "               device: torch.device):\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        logits = model(X)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        preds = torch.argmax(probs, dim=-1)\n",
    "        \n",
    "        loss = loss_fn(logits, y)\n",
    "        acc = acc_fn(preds, y)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 30 == 0:\n",
    "            print(f'train loss: {loss:.4f} train acc: {acc:.2f}%')\n",
    "            \n",
    "\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc = (train_acc / len(dataloader)) * 100\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5c6978e-07f6-435f-b1a5-bc7c6cdea67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: nn.Module,\n",
    "              loss_fn: nn.Module,\n",
    "              acc_fn,\n",
    "              dataloader: torch.utils.data,\n",
    "              device: torch.device):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            test_logits = model(X)\n",
    "            test_preds = torch.argmax(test_logits, dim=1)\n",
    "            \n",
    "            test_loss += loss_fn(test_logits, y).item()\n",
    "            test_acc += acc_fn(test_preds, y).item()\n",
    "\n",
    "            if batch % 30 == 0:\n",
    "                print(f'test loss: {test_loss:.4f} test acc: {test_acc:.2f}%')\n",
    "\n",
    "        test_loss /= len(dataloader)\n",
    "        test_acc = (test_acc / len(dataloader)) * 100\n",
    "\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee42d88e-5ce2-4274-91e9-14fc09624831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model: nn.Module,\n",
    "             loss_fn: nn.Module,\n",
    "             optimizer: torch.optim,\n",
    "             train_dataloader: torch.utils.data,\n",
    "             test_dataloader: torch.utils.data,\n",
    "             acc_fn,\n",
    "             epochs: int,\n",
    "             device: torch.device):\n",
    "\n",
    "    results = {\n",
    "        'train_loss' : [],\n",
    "        'train_acc' : [],\n",
    "        'test_loss' : [],\n",
    "        'test_acc' : []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch: {epoch}\\n--------')\n",
    "\n",
    "        train_loss, train_acc = train_step(model=model, loss_fn=loss_fn, optimizer=optimizer, acc_fn=acc_fn, dataloader=train_dataloader, device=device)\n",
    "        results['train_loss'].append(train_loss)\n",
    "        results['train_acc'].append(train_acc)\n",
    "\n",
    "        print()\n",
    "        \n",
    "        test_loss, test_acc = test_step(model=model, loss_fn=loss_fn, acc_fn=acc_fn, dataloader=test_dataloader, device=device)\n",
    "        results['test_loss'].append(test_loss)\n",
    "        results['test_acc'].append(test_acc)\n",
    "\n",
    "        \n",
    "        print(f'train loss: {train_loss:.4f} | test loss: {test_loss:.4f}')\n",
    "        print(f'train acc: {train_acc:.2f}% | test acc: {test_acc:.2f}%')\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37e27c1f-ffbe-48de-b154-6b027d760810",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentClassifier(embedding_layer=roberta, embedding_dim=conf.EMBEDDING_DIM, hidden_units=conf.HIDDEN_UNITS)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=conf.LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6406ffc4-5822-49b5-ad11-2f63a2e1b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_set = CustomDataset(reviews[:2], labels[:2], transforms)\n",
    "demo_dataloader = DataLoader(dataset=demo_set, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd09fad-f47a-48c3-b7f7-b7f8e29ed019",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = training(model=model,\n",
    "                   loss_fn=conf.LOSS_FN,\n",
    "                   optimizer=optimizer,\n",
    "                   train_dataloader=demo_dataloader,\n",
    "                   test_dataloader=demo_dataloader,\n",
    "                   acc_fn=conf.ACC_FN,\n",
    "                   epochs=conf.EPOCHS,\n",
    "                   device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6bd92a-2783-4890-9157-e0e7bf0c40ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon-kernel",
   "language": "python",
   "name": "hackathon-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
